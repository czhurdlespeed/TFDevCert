{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e16541f2-e6cf-4301-8135-3914a59ead59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "from torchviz import make_dot\n",
    "\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "import random\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42571d34-8743-464e-ab6f-a78877930a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(list_of_epochs, list_of_train_losses, list_of_train_accuracies, list_of_val_accuracies):\n",
    "    plt.figure(figsize=(20, 9))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(list_of_epochs, list_of_train_losses, label='training loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(list_of_epochs, list_of_train_accuracies, label='training accuracy')\n",
    "    plt.plot(list_of_epochs, list_of_val_accuracies, label='validation accuracy')\n",
    "    plt.legend()\n",
    "    if not os.path.isdir('./result_plots'):\n",
    "        os.makedirs('./result_plots')\n",
    "    plt.savefig('./result_plots/accuracy_plot_per_epoch.jpg')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "477146b5-2988-498a-9bf0-eb50f924f531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_lr(optim, epoch_num, lrate):\n",
    "    lrate = lrate * (0.1 ** (epoch_num // 20))\n",
    "    for params in optim.param_groups:\n",
    "        params['lr'] = lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a06cae87-fadd-467b-abc9-62a6027a9839",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, optim, loss_func, epoch_num, lrate):\n",
    "    model.train()\n",
    "    loop_iter = 0\n",
    "    training_loss = 0\n",
    "    training_accuracy = 0\n",
    "    for training_data, training_label in train_dataloader:\n",
    "        set_lr(optim, epoch_num, lrate)\n",
    "        training_data, training_label = training_data.to(device), training_label.to(device)\n",
    "        optim.zero_grad()\n",
    "        pred_raw = model(training_data)\n",
    "        curr_loss = loss_func(pred_raw, training_label)\n",
    "        curr_loss.backward()\n",
    "        optim.step()\n",
    "        training_loss += curr_loss.data\n",
    "        pred = pred_raw.data.max(1)[1]\n",
    "\n",
    "        curr_accuracy = float(pred.eq(training_label.data).sum()) * 100. / len(training_data) # avg loss per sample in batch\n",
    "        training_accuracy += curr_accuracy\n",
    "        loop_iter += 1\n",
    "        if loop_iter % 100 == 0:\n",
    "            print(f\"epoch {epoch_num}, loss: {curr_loss.data}, accuracy: {curr_accuracy}\")\n",
    "\n",
    "    data_size = len(train_dataloader.dataset) // batch_size\n",
    "    return training_loss / data_size, training_accuracy / data_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e118827d-eac6-4ab0-b58b-1a4a4b6cd4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, test_data_loader):\n",
    "    model.eval()\n",
    "    success = 0\n",
    "    with torch.no_grad():\n",
    "        for test_data, test_label in test_data_loader:\n",
    "            test_data, test_label = test_data.to(device), test_label.to(device)\n",
    "            pred_raw = model(test_data)\n",
    "            pred = pred_raw.data.max(1)[1]\n",
    "            success += pred.eq(test_label.data).sum()\n",
    "\n",
    "    return float(success) * 100. / len(test_data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bd25b33-7469-491e-815f-6e91d7aacd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "graph_probability = 0.7\n",
    "node_channel_count = 64\n",
    "num_nodes = 16\n",
    "lrate = 0.1\n",
    "batch_size = 64\n",
    "train_mode = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "184f5e30-2559-4b4c-ac28-bb2f2dc48dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(batch_size):\n",
    "    transform_train_dataset = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4983, 0.4795, 0.4382), (0.2712, 0.2602, 0.2801)),\n",
    "    ])\n",
    "\n",
    "    transform_test_dataset = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4983, 0.4795, 0.4382), (0.2712, 0.2602, 0.2801)),\n",
    "    ])\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10('dataset', transform=transform_train_dataset, train=True, download=True),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10('dataset', transform=transform_test_dataset, train=False),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "    return train_dataloader, test_dataloader\n",
    "train_dataloader, test_dataloader = load_dataset(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1517216-e46d-452b-84a0-388beb94bc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RndGraph:\n",
    "    def __init__(self, num_nodes, graph_probability, nearest_neighbor_k=4, num_edges_attach=5):\n",
    "        self.num_nodes = num_nodes\n",
    "        self.graph_probability = graph_probability\n",
    "        self.nearest_neighbor_k = nearest_neighbor_k\n",
    "        self.num_edges_attach = num_edges_attach\n",
    "\n",
    "    def make_graph_obj(self):\n",
    "        graph_obj = nx.random_graphs.connected_watts_strogatz_graph(self.num_nodes,\n",
    "                                                                    self.nearest_neighbor_k,\n",
    "                                                                    self.graph_probability)\n",
    "        return graph_obj\n",
    "\n",
    "    def get_graph_config(self, graph_obj):\n",
    "        incoming_edges = {}\n",
    "        incoming_edges[0] = []\n",
    "        node_list = [0]\n",
    "        last = []\n",
    "        for n in graph_obj.nodes(): # loop through nodes of graph\n",
    "            neighbor_list = list(graph_obj.neighbors(n))\n",
    "            neighbor_list.sort()\n",
    "\n",
    "            edge_list = []\n",
    "            passed_list = []\n",
    "            for nbr in neighbor_list:\n",
    "                if n > nbr:\n",
    "                    edge_list.append(nbr + 1) # incoming edges can only come from lower nodes than current (n)\n",
    "                    passed_list.append(nbr)\n",
    "            if not edge_list:\n",
    "                edge_list.append(0)\n",
    "            incoming_edges[n+1] = edge_list\n",
    "            if passed_list == neighbor_list:\n",
    "                last.append(n + 1) # sink found\n",
    "            node_list.append(n + 1)\n",
    "        incoming_edges[self.num_nodes+1] = last\n",
    "        node_list.append(self.num_nodes+1)\n",
    "        return node_list, incoming_edges\n",
    "    def save_graph(self, graph_obj, path_to_write):\n",
    "        if not os.path.isdir(\"cached_graph_obj\"):\n",
    "            os.mkdir(\"cached_graph_obj\")\n",
    "        with open(f\"./cached_graph_obj/{path_to_write}\", \"w\") as fh:\n",
    "            yaml.dump(graph_obj, fh)\n",
    "    def load_graph(self, path_to_read):\n",
    "        with open(f\"./cached_graph_obj/{path_to_read}\", \"r\") as fh:\n",
    "            return yaml.load(fh, Loader=yaml.Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34646836-6b0e-43e9-b39e-43684c73f1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weigths(layer):\n",
    "    if isinstance(layer, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform_(layer.weight)\n",
    "        if layer.bias is not None:\n",
    "            torch.nn.init.zeros_(layer.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ac65f59-86d8-4a88-9072-98b8d880687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SepConv2d(nn.Module):\n",
    "    def __init__(self, input_ch, output_ch, kernel_length=3, dilation_size=1,\n",
    "                 padding_size=1, stride_length=1, bias_flag=True):\n",
    "        super().__init__()\n",
    "        self.conv_layer = nn.Conv2d(input_ch, input_ch, kernel_length, \n",
    "                                    stride_length, padding_size, dilation_size,\n",
    "                                    bias=bias_flag, groups=input_ch)\n",
    "        self.pointwise_layer = nn.Conv2d(input_ch, output_ch, kernel_size=1, stride=1, padding=0, dilation=1,\n",
    "                                        groups=1, bias=bias_flag)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pointwise_layer(self.conv_layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea8ac767-a93c-4406-b8f3-b6f76162e5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnitLayer(nn.Module):\n",
    "    def __init__(self, input_ch, output_ch, stride_length=1):\n",
    "        super().__init__()\n",
    "        self.dropout = 0.3\n",
    "        self.unit_layer = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            SepConv2d(input_ch, output_ch, stride_length=stride_length),\n",
    "            nn.BatchNorm2d(output_ch),\n",
    "            nn.Dropout(self.dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.unit_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1ec131d-10ab-4b72-9473-eafe2e64dcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphNode(nn.Module):\n",
    "    def __init__(self, input_degree, input_ch, output_ch, stride_length=1):\n",
    "        super(GraphNode, self).__init__()\n",
    "        self.input_degree = input_degree\n",
    "        if len(self.input_degree) > 1:\n",
    "            self.params = nn.Parameter(torch.ones(len(self.input_degree), requires_grad=True))\n",
    "        self.unit_layer = UnitLayer(input_ch, output_ch, stride_length=stride_length)\n",
    "\n",
    "    def forward(self, *ip):\n",
    "        if len(self.input_degree) > 1:\n",
    "            op = (ip[0] * torch.sigmoid(self.params[0]))\n",
    "            for idx in range(1, len(ip)):\n",
    "                op += (ip[idx] * torch.sigmoid(self.params[idx]))\n",
    "            return self.unit_layer(op)\n",
    "        else:\n",
    "            return self.unit_layer(ip[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bf70e79-c3ef-4719-a0e9-849d44276203",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandWireGraph(nn.Module):\n",
    "    def __init__(self, num_nodes, graph_prob, input_ch, output_ch, train_mode, graph_name):\n",
    "        super(RandWireGraph, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.graph_prob = graph_prob\n",
    "        self.input_ch = input_ch\n",
    "        self.output_ch = output_ch\n",
    "        self.train_mode = train_mode\n",
    "        self.graph_name = graph_name\n",
    "\n",
    "        # get graph nodes and in edges\n",
    "        rnd_graph_node = RndGraph(self.num_nodes, self.graph_prob)\n",
    "        if self.train_mode is True:\n",
    "            print(\"train_mode: ON\")\n",
    "            rnd_graph = rnd_graph_node.make_graph_obj()\n",
    "            self.node_list, self.incoming_edge_list = rnd_graph_node.get_graph_config(rnd_graph)\n",
    "            rnd_graph_node.save_graph(rnd_graph, graph_name)\n",
    "        else:\n",
    "            rnd_graph = rnd_graph_node.load_graph(graph_name)\n",
    "            self.node_list, self.incoming_edge_list = rnd_graph_node.get_graph_config(rnd_graph)\n",
    "\n",
    "        # define input Node\n",
    "        self.list_of_modules = nn.ModuleList([GraphNode(self.incoming_edge_list[0], self.input_ch, self.output_ch, \n",
    "                                                        stride_length=2)])\n",
    "        # define the rest Node\n",
    "        self.list_of_modules.extend([GraphNode(self.incoming_edge_list[n], self.output_ch, self.output_ch) \n",
    "                                     for n in self.node_list if n > 0])\n",
    "\n",
    "    def forward(self, x):\n",
    "        mem_dict = {}\n",
    "        # start vertex\n",
    "        op = self.list_of_modules[0].forward(x)\n",
    "        mem_dict[0] = op\n",
    "\n",
    "        # the rest vertex\n",
    "        for n in range(1, len(self.node_list) - 1):\n",
    "            # print(node, self.in_edges[node][0], self.in_edges[node])\n",
    "            if len(self.incoming_edge_list[n]) > 1:\n",
    "                op = self.list_of_modules[n].forward(*[mem_dict[incoming_vtx] \n",
    "                                                       for incoming_vtx in self.incoming_edge_list[n]])\n",
    "            else:\n",
    "                op = self.list_of_modules[n].forward(mem_dict[self.incoming_edge_list[n][0]])\n",
    "            mem_dict[n] = op\n",
    "            \n",
    "        op = mem_dict[self.incoming_edge_list[self.num_nodes + 1][0]]\n",
    "        for incoming_vtx in range(1, len(self.incoming_edge_list[self.num_nodes + 1])):\n",
    "            op += mem_dict[self.incoming_edge_list[self.num_nodes + 1][incoming_vtx]]\n",
    "        return op / len(self.incoming_edge_list[self.num_nodes + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bde2443c-5b28-40e0-bbf8-fdcac6c8ccf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandWireNNModel(nn.Module):\n",
    "    def __init__(self, num_nodes, graph_prob, input_ch, output_ch, train_mode):\n",
    "        super(RandWireNNModel, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.graph_prob = graph_prob\n",
    "        self.input_ch = input_ch\n",
    "        self.output_ch = output_ch\n",
    "        self.train_mode = train_mode\n",
    "        self.dropout = 0.3\n",
    "        self.class_num = 10\n",
    "            \n",
    "        self.conv_layer_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=self.output_ch, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(self.output_ch),\n",
    "        )\n",
    "\n",
    "        self.conv_layer_2 = nn.Sequential(\n",
    "            RandWireGraph(self.num_nodes, self.graph_prob, self.input_ch, self.output_ch*2, self.train_mode, \n",
    "                          graph_name=\"conv_layer_2\")\n",
    "        )\n",
    "        self.conv_layer_3 = nn.Sequential(\n",
    "            RandWireGraph(self.num_nodes, self.graph_prob, self.input_ch*2, self.output_ch*4, self.train_mode, \n",
    "                          graph_name=\"conv_layer_3\")\n",
    "        )\n",
    "        self.conv_layer_4 = nn.Sequential(\n",
    "            RandWireGraph(self.num_nodes, self.graph_prob, self.input_ch*4, self.output_ch*8, self.train_mode, \n",
    "                          graph_name=\"conv_layer_4\")\n",
    "        )\n",
    "\n",
    "        self.classifier_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.input_ch*8, out_channels=1280, kernel_size=1),\n",
    "            nn.BatchNorm2d(1280)\n",
    "        )\n",
    "\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(1280, self.class_num)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer_1(x)\n",
    "        x = self.conv_layer_2(x)\n",
    "        x = self.conv_layer_3(x)\n",
    "        x = self.conv_layer_4(x)\n",
    "        x = self.classifier_layer(x)\n",
    "\n",
    "        # global average pooling\n",
    "        _, _, h, w = x.size()\n",
    "        x = F.avg_pool2d(x, kernel_size=[h, w])\n",
    "        x = torch.squeeze(x)\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38ce12c9-318b-4f8e-8056-99284deebf74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_mode: ON\n",
      "train_mode: ON\n",
      "train_mode: ON\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     14\u001b[0m     epochs\u001b[38;5;241m.\u001b[39mappend(ep)\n\u001b[0;32m---> 15\u001b[0m     training_loss, training_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrand_wire_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlrate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     test_accuracy \u001b[38;5;241m=\u001b[39m accuracy(rand_wire_model, test_dataloader)\n\u001b[1;32m     17\u001b[0m     test_accuracies\u001b[38;5;241m.\u001b[39mappend(test_accuracy)\n",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, optim, loss_func, epoch_num, lrate)\u001b[0m\n\u001b[1;32m      8\u001b[0m training_data, training_label \u001b[38;5;241m=\u001b[39m training_data\u001b[38;5;241m.\u001b[39mto(device), training_label\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 10\u001b[0m pred_raw \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m curr_loss \u001b[38;5;241m=\u001b[39m loss_func(pred_raw, training_label)\n\u001b[1;32m     12\u001b[0m curr_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/DLplayground/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/DLplayground/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 43\u001b[0m, in \u001b[0;36mRandWireNNModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_layer_1(x)\n\u001b[1;32m     42\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_layer_2(x)\n\u001b[0;32m---> 43\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_layer_3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_layer_4(x)\n\u001b[1;32m     45\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier_layer(x)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/DLplayground/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/DLplayground/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/DLplayground/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/DLplayground/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/DLplayground/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 39\u001b[0m, in \u001b[0;36mRandWireGraph.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_list) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# print(node, self.in_edges[node][0], self.in_edges[node])\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mincoming_edge_list[n]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 39\u001b[0m         op \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_of_modules\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmem_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mincoming_vtx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m                                               \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mincoming_vtx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincoming_edge_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m         op \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlist_of_modules[n]\u001b[38;5;241m.\u001b[39mforward(mem_dict[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mincoming_edge_list[n][\u001b[38;5;241m0\u001b[39m]])\n",
      "Cell \u001b[0;32mIn[12], line 14\u001b[0m, in \u001b[0;36mGraphNode.forward\u001b[0;34m(self, *ip)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(ip)):\n\u001b[1;32m     13\u001b[0m         op \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (ip[idx] \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[idx]))\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munit_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_layer(ip[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/DLplayground/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/DLplayground/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 12\u001b[0m, in \u001b[0;36mUnitLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munit_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/DLplayground/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/DLplayground/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/DLplayground/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/DLplayground/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/DLplayground/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/DLplayground/lib/python3.11/site-packages/torch/nn/modules/dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/DLplayground/lib/python3.11/site-packages/torch/nn/functional.py:1268\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1268\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rand_wire_model = RandWireNNModel(num_nodes, graph_probability, node_channel_count, node_channel_count, train_mode).to(device)\n",
    "\n",
    "optim_module = optim.SGD(rand_wire_model.parameters(), lr=lrate, weight_decay=1e-4, momentum=0.8)\n",
    "loss_func = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "epochs = []\n",
    "test_accuracies = []\n",
    "training_accuracies = []\n",
    "training_losses = []\n",
    "best_test_accuracy = 0\n",
    "\n",
    "start_time = time.time()\n",
    "for ep in range(1, num_epochs + 1):\n",
    "    epochs.append(ep)\n",
    "    training_loss, training_accuracy = train(rand_wire_model, train_dataloader, optim_module, loss_func, ep, lrate)\n",
    "    test_accuracy = accuracy(rand_wire_model, test_dataloader)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    training_losses.append(training_loss)\n",
    "    training_accuracies.append(training_accuracy)\n",
    "    print('test acc: {0:.2f}%, best test acc: {1:.2f}%'.format(test_accuracy, best_test_accuracy))\n",
    "\n",
    "    if best_test_accuracy < test_accuracy:\n",
    "        model_state = {\n",
    "            'model': rand_wire_model.state_dict(),\n",
    "            'accuracy': test_accuracy,\n",
    "            'ep': ep,\n",
    "        }\n",
    "        if not os.path.isdir('model_checkpoint'):\n",
    "            os.mkdir('model_checkpoint')\n",
    "        model_filename = \"ch_count_\" + str(node_channel_count) + \"_prob_\" + str(graph_probability)\n",
    "        torch.save(model_state, './model_checkpoint/' + model_filename + 'ckpt.t7')\n",
    "        best_test_accuracy = test_accuracy\n",
    "        plot_results(epochs, training_losses, training_accuracies, test_accuracies)\n",
    "    print(\"model train time: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf85608-77d4-4376-9d4c-20f38b812e91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
