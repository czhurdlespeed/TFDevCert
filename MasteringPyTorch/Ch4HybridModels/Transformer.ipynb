{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5595931a-121c-440a-964c-adf07f649460",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/calwetzel/.pyenv/versions/3.11.7/envs/DLplayground/lib/python3.11/site-packages/torchtext/datasets/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/Users/calwetzel/.pyenv/versions/3.11.7/envs/DLplayground/lib/python3.11/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/Users/calwetzel/.pyenv/versions/3.11.7/envs/DLplayground/lib/python3.11/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/Users/calwetzel/.pyenv/versions/3.11.7/envs/DLplayground/lib/python3.11/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "from torchtext.datasets import PennTreebank\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "673f912e-4fc3-47fc-a462-1d5dc30cdbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sqr_nxt_mask(size):\n",
    "    # Create mask and move it to correct device\n",
    "    mask = torch.triu(torch.ones(size, size), diagonal=1)\n",
    "    mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "    mask = mask.masked_fill(mask == 0, 0.0)  # Add this line\n",
    "    return mask\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_token, num_inputs, num_heads,\n",
    "                 num_hidden, num_layers, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.model_name = 'transformer'\n",
    "        self.position_enc = PosEnc(num_inputs, dropout)\n",
    "        layers_enc = TransformerEncoderLayer(num_inputs, num_heads, \n",
    "                                           num_hidden, dropout,\n",
    "                                           batch_first=True)  # Note this is True\n",
    "        self.enc_transformer = TransformerEncoder(layers_enc, num_layers)\n",
    "        self.enc = nn.Embedding(num_token, num_inputs)\n",
    "        self.num_inputs = num_inputs\n",
    "        self.dec = nn.Linear(num_inputs, num_token)\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        initial_rng = 0.12\n",
    "        self.enc.weight.data.uniform_(-initial_rng, initial_rng)\n",
    "        self.dec.bias.data.zero_()\n",
    "        self.dec.weight.data.uniform_(-initial_rng, initial_rng)\n",
    "\n",
    "    def forward(self, source, mask_source):\n",
    "        source = self.enc(source) * math.sqrt(self.num_inputs)\n",
    "        source = self.position_enc(source)\n",
    "        op = self.enc_transformer(source, mask_source)\n",
    "        op = self.dec(op)\n",
    "        return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5606d65f-3b12-40f5-8ee6-dbd6cc8ecf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosEnc(nn.Module):\n",
    "    def __init__(self, d_m, dropout=0.2, size_limit=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        p_enc = torch.zeros(size_limit, 1, d_m)\n",
    "        pos = torch.arange(size_limit, dtype=torch.float).unsqueeze(1)\n",
    "        divider = torch.exp(torch.arange(0, d_m, 2).float() * (-math.log(10_000.0) / d_m))\n",
    "        p_enc[:, 0, 0::2] = torch.sin(pos * divider)\n",
    "        p_enc[:, 0, 1::2] = torch.cos(pos * divider)\n",
    "        self.register_buffer('p_enc', p_enc)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(x + self.p_enc[:x.size(0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d116d8c-b83b-4826-acfa-b35c802d6dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/calwetzel/.pyenv/versions/3.11.7/envs/DLplayground/lib/python3.11/site-packages/torchdata/datapipes/__init__.py:18: UserWarning: \n",
      "################################################################################\n",
      "WARNING!\n",
      "The 'datapipes', 'dataloader2' modules are deprecated and will be removed in a\n",
      "future torchdata release! Please see https://github.com/pytorch/data/issues/1196\n",
      "to learn more and leave feedback.\n",
      "################################################################################\n",
      "\n",
      "  deprecation_warning()\n"
     ]
    }
   ],
   "source": [
    "# Use raw text instead of iterator\n",
    "tr_iter = PennTreebank(split='train')\n",
    "tr_raw = [' '.join(text) for text in tr_iter]  # Convert to raw text\n",
    "\n",
    "# Tokenize and build vocabulary\n",
    "tkzer = get_tokenizer('basic_english')\n",
    "vocabulary = build_vocab_from_iterator([tkzer(text) for text in tr_raw], specials=['<unk>'])\n",
    "vocabulary.set_default_index(vocabulary['<unk>'])\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Process data function\n",
    "def process_data(raw_text):\n",
    "    numericalised_text = [torch.tensor(vocabulary(tkzer(text)), dtype=torch.long) for text in raw_text]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, numericalised_text)))\n",
    "\n",
    "# Get all splits and convert to raw text\n",
    "tr_iter = PennTreebank(split='train')\n",
    "val_iter = PennTreebank(split='valid')\n",
    "te_iter = PennTreebank(split='test')\n",
    "\n",
    "tr_raw = [' '.join(text) for text in tr_iter]\n",
    "val_raw = [' '.join(text) for text in val_iter]\n",
    "te_raw = [' '.join(text) for text in te_iter]\n",
    "\n",
    "# Process all splits\n",
    "training_text = process_data(tr_raw)\n",
    "validation_text = process_data(val_raw)\n",
    "testing_text = process_data(te_raw)\n",
    "\n",
    "# Batch generation function\n",
    "def gen_batches(text_dataset, batch_size):\n",
    "    num_batches = text_dataset.size(0) // batch_size\n",
    "    text_dataset = text_dataset[:num_batches * batch_size]\n",
    "    text_dataset = text_dataset.view(batch_size, num_batches).t().contiguous()\n",
    "    return text_dataset.to(device)\n",
    "\n",
    "# Generate batches\n",
    "training_batch_size = 32\n",
    "evaluation_batch_size = 16\n",
    "training_data = gen_batches(training_text, training_batch_size)\n",
    "validation_data = gen_batches(validation_text, evaluation_batch_size)\n",
    "testing_data = gen_batches(testing_text, evaluation_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1accaf7f-fc2d-47a5-8250-a3a65d5a053d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 64\n",
    "def return_batch(src, k): \n",
    "    sequence_length = min(max_seq_len, len(src) - 1 - k)\n",
    "    sequence_data = src[k:k+sequence_length]\n",
    "    sequence_label = src[k+1:k+1+sequence_length].reshape(-1)\n",
    "    return sequence_data, sequence_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d76f31fc-51b3-4ea7-964c-3d0be8a3420b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = len(vocabulary)\n",
    "embedding_size = 256\n",
    "num_hidden_params = 256\n",
    "num_layers = 2\n",
    "num_heads = 2\n",
    "dropout = 0.25 \n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "lrate = 4.0\n",
    "transformer_model = Transformer(num_tokens, embedding_size, num_heads,\n",
    "                                num_hidden_params, num_layers,\n",
    "                                dropout).to(device)\n",
    "optim_module = torch.optim.SGD(transformer_model.parameters(), lr=lrate)\n",
    "sched_module = torch.optim.lr_scheduler.StepLR(optim_module, 1.0, gamma=0.88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0895d734-3d68-4fdd-a3a5-8e4eda16f0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sqr_nxt_mask(size):\n",
    "    mask = torch.triu(torch.ones(size, size), diagonal=1)\n",
    "    mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "    mask = mask.masked_fill(mask == 0, 0.0)\n",
    "    return mask\n",
    "\n",
    "def train_model():\n",
    "    transformer_model.train()\n",
    "    loss_total = 0.\n",
    "    time_start = time.time()\n",
    "    \n",
    "    for b, i in enumerate(range(0, training_data.size(0) - 1, max_seq_len)):\n",
    "        train_data_batch, train_label_batch = return_batch(training_data, i)\n",
    "        \n",
    "        # Use size(0) if 1D tensor, otherwise use size(1) for 2D\n",
    "        if train_data_batch.dim() == 1:\n",
    "            sequence_length = train_data_batch.size(0)\n",
    "        else:\n",
    "            sequence_length = train_data_batch.size(1)  # For batch_first=True\n",
    "            \n",
    "        mask_source = gen_sqr_nxt_mask(sequence_length).to(device)\n",
    "        \n",
    "        op = transformer_model(train_data_batch, mask_source)\n",
    "        loss_curr = loss_func(op.view(-1, num_tokens), train_label_batch)\n",
    "        optim_module.zero_grad()\n",
    "        loss_curr.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(transformer_model.parameters(), 0.6)\n",
    "        optim_module.step()\n",
    "        \n",
    "        loss_total += loss_curr.item()\n",
    "        interval = 100\n",
    "        if b % interval == 0 and b > 0:\n",
    "            loss_interval = loss_total / interval\n",
    "            time_delta = time.time() - time_start\n",
    "            print(f\"epoch {ep}, {b}/{len(training_data)//max_seq_len} batches, \"\n",
    "                  f\"training loss {loss_interval:.2f}, \"\n",
    "                  f\"training perplexity {math.exp(loss_interval):.2f}\")\n",
    "            loss_total = 0\n",
    "            time_start = time.time()\n",
    "\n",
    "def eval_model(eval_model_obj, eval_data_source):\n",
    "    eval_model_obj.eval() \n",
    "    loss_total = 0.\n",
    "    mask_source = gen_sqr_nxt_mask(max_seq_len).to(device)\n",
    "    with torch.no_grad():\n",
    "        for j in range(0, eval_data_source.size(0) - 1, max_seq_len):\n",
    "            eval_data, eval_label = return_batch(eval_data_source, j)\n",
    "            sequence_length = eval_data.size(0)\n",
    "            if sequence_length != max_seq_len:\n",
    "                mask_source = mask_source[:sequence_length, :sequence_length]\n",
    "            op = eval_model_obj(eval_data, mask_source)\n",
    "            op_flat = op.view(-1, num_tokens)\n",
    "            loss_total += sequence_length * loss_func(op_flat, eval_label).item()\n",
    "    return loss_total / (len(eval_data_source) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f59ac55-4bc5-4eef-91df-8d852b594a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, 100/2016 batches, training loss 3.77, training perplexity 43.44\n",
      "epoch 1, 200/2016 batches, training loss 2.61, training perplexity 13.58\n",
      "epoch 1, 300/2016 batches, training loss 2.58, training perplexity 13.20\n",
      "epoch 1, 400/2016 batches, training loss 2.58, training perplexity 13.14\n",
      "epoch 1, 500/2016 batches, training loss 2.56, training perplexity 12.91\n",
      "epoch 1, 600/2016 batches, training loss 2.56, training perplexity 12.89\n",
      "epoch 1, 700/2016 batches, training loss 2.55, training perplexity 12.80\n",
      "epoch 1, 800/2016 batches, training loss 2.55, training perplexity 12.85\n",
      "epoch 1, 900/2016 batches, training loss 2.55, training perplexity 12.82\n",
      "epoch 1, 1000/2016 batches, training loss 2.54, training perplexity 12.74\n",
      "epoch 1, 1100/2016 batches, training loss 2.55, training perplexity 12.77\n",
      "epoch 1, 1200/2016 batches, training loss 2.54, training perplexity 12.72\n",
      "epoch 1, 1300/2016 batches, training loss 2.54, training perplexity 12.70\n",
      "epoch 1, 1400/2016 batches, training loss 2.54, training perplexity 12.67\n",
      "epoch 1, 1500/2016 batches, training loss 2.55, training perplexity 12.84\n",
      "epoch 1, 1600/2016 batches, training loss 2.55, training perplexity 12.82\n",
      "epoch 1, 1700/2016 batches, training loss 2.54, training perplexity 12.74\n",
      "epoch 1, 1800/2016 batches, training loss 2.55, training perplexity 12.77\n",
      "epoch 1, 1900/2016 batches, training loss 2.54, training perplexity 12.67\n",
      "epoch 1, 2000/2016 batches, training loss 2.53, training perplexity 12.60\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 1, 16, 16]' is invalid for input of size 4096",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m ep_time_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      7\u001b[0m train_model()\n\u001b[0;32m----> 8\u001b[0m validation_loss \u001b[38;5;241m=\u001b[39m \u001b[43meval_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformer_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, validation loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalidation_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, validation perplexity \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmath\u001b[38;5;241m.\u001b[39mexp(validation_loss)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 51\u001b[0m, in \u001b[0;36meval_model\u001b[0;34m(eval_model_obj, eval_data_source)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sequence_length \u001b[38;5;241m!=\u001b[39m max_seq_len:\n\u001b[1;32m     50\u001b[0m     mask_source \u001b[38;5;241m=\u001b[39m mask_source[:sequence_length, :sequence_length]\n\u001b[0;32m---> 51\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43meval_model_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_source\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m op_flat \u001b[38;5;241m=\u001b[39m op\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, num_tokens)\n\u001b[1;32m     53\u001b[0m loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m sequence_length \u001b[38;5;241m*\u001b[39m loss_func(op_flat, eval_label)\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/DLplayground/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1529\u001b[0m     backward_pre_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_backward_pre_hooks()\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks:\n\u001b[0;32m-> 1532\u001b[0m     full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_backward_hooks()\n\u001b[1;32m   1534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks:\n\u001b[1;32m   1535\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1536\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_pre_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1537\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1538\u001b[0m     ):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/DLplayground/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks_with_kwargs:\n\u001b[1;32m   1540\u001b[0m     args_kwargs_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m args_kwargs_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1542\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args_kwargs_result, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args_kwargs_result) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1543\u001b[0m             args, kwargs \u001b[38;5;241m=\u001b[39m args_kwargs_result\n",
      "Cell \u001b[0;32mIn[2], line 32\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, source, mask_source)\u001b[0m\n\u001b[1;32m     30\u001b[0m source \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menc(source) \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_inputs)\n\u001b[1;32m     31\u001b[0m source \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_enc(source)\n\u001b[0;32m---> 32\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menc_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_source\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec(op)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/DLplayground/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1529\u001b[0m     backward_pre_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_backward_pre_hooks()\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks:\n\u001b[0;32m-> 1532\u001b[0m     full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_backward_hooks()\n\u001b[1;32m   1534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks:\n\u001b[1;32m   1535\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1536\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_pre_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1537\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1538\u001b[0m     ):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/DLplayground/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks_with_kwargs:\n\u001b[1;32m   1540\u001b[0m     args_kwargs_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m args_kwargs_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1542\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args_kwargs_result, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args_kwargs_result) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1543\u001b[0m             args, kwargs \u001b[38;5;241m=\u001b[39m args_kwargs_result\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/DLplayground/lib/python3.11/site-packages/torch/nn/modules/transformer.py:415\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTransformerDecoder\u001b[39;00m(Module):\n\u001b[1;32m    403\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"TransformerDecoder is a stack of N decoder layers.\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \n\u001b[1;32m    405\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;124;03m        decoder_layer: an instance of the TransformerDecoderLayer() class (required).\u001b[39;00m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;124;03m        num_layers: the number of sub-decoder-layers in the decoder (required).\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;124;03m        norm: the layer normalization component (optional).\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \n\u001b[1;32m    410\u001b[0m \u001b[38;5;124;03m    Examples::\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;124;03m        >>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\u001b[39;00m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;124;03m        >>> transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\u001b[39;00m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;124;03m        >>> memory = torch.rand(10, 32, 512)\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;124;03m        >>> tgt = torch.rand(20, 32, 512)\u001b[39;00m\n\u001b[0;32m--> 415\u001b[0m \u001b[38;5;124;03m        >>> out = transformer_decoder(tgt, memory)\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    418\u001b[0m     __constants__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnorm\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, decoder_layer, num_layers, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/DLplayground/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1529\u001b[0m     backward_pre_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_backward_pre_hooks()\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks:\n\u001b[0;32m-> 1532\u001b[0m     full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_backward_hooks()\n\u001b[1;32m   1534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks:\n\u001b[1;32m   1535\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1536\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_pre_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1537\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1538\u001b[0m     ):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/DLplayground/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks_with_kwargs:\n\u001b[1;32m   1540\u001b[0m     args_kwargs_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m args_kwargs_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1542\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args_kwargs_result, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args_kwargs_result) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1543\u001b[0m             args, kwargs \u001b[38;5;241m=\u001b[39m args_kwargs_result\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/DLplayground/lib/python3.11/site-packages/torch/nn/modules/transformer.py:719\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    715\u001b[0m         x = self.norm2(x + self._ff_block(x))\n\u001b[1;32m    717\u001b[0m     return x\n\u001b[0;32m--> 719\u001b[0m # self-attention block\n\u001b[1;32m    720\u001b[0m def _sa_block(self, x: Tensor,\n\u001b[1;32m    721\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: bool = False) -> Tensor:\n\u001b[1;32m    722\u001b[0m     x = self.self_attn(x, x, x,\n\u001b[1;32m    723\u001b[0m                        attn_mask=attn_mask,\n\u001b[1;32m    724\u001b[0m                        key_padding_mask=key_padding_mask,\n\u001b[1;32m    725\u001b[0m                        need_weights=False, is_causal=is_causal)[0]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/envs/DLplayground/lib/python3.11/site-packages/torch/nn/modules/activation.py:1314\u001b[0m, in \u001b[0;36mmerge_masks\u001b[0;34m(self, attn_mask, key_padding_mask, query)\u001b[0m\n\u001b[1;32m   1299\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mPReLU\u001b[39;00m(Module):\n\u001b[1;32m   1300\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Applies the element-wise function:\u001b[39;00m\n\u001b[1;32m   1301\u001b[0m \n\u001b[1;32m   1302\u001b[0m \u001b[38;5;124;03m    .. math::\u001b[39;00m\n\u001b[1;32m   1303\u001b[0m \u001b[38;5;124;03m        \\text{PReLU}(x) = \\max(0,x) + a * \\min(0,x)\u001b[39;00m\n\u001b[1;32m   1304\u001b[0m \n\u001b[1;32m   1305\u001b[0m \u001b[38;5;124;03m    or\u001b[39;00m\n\u001b[1;32m   1306\u001b[0m \n\u001b[1;32m   1307\u001b[0m \u001b[38;5;124;03m    .. math::\u001b[39;00m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;124;03m        \\text{PReLU}(x) =\u001b[39;00m\n\u001b[1;32m   1309\u001b[0m \u001b[38;5;124;03m        \\begin{cases}\u001b[39;00m\n\u001b[1;32m   1310\u001b[0m \u001b[38;5;124;03m        x, & \\text{ if } x \\geq 0 \\\\\u001b[39;00m\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;124;03m        ax, & \\text{ otherwise }\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;124;03m        \\end{cases}\u001b[39;00m\n\u001b[1;32m   1313\u001b[0m \n\u001b[0;32m-> 1314\u001b[0m \u001b[38;5;124;03m    Here :math:`a` is a learnable parameter. When called without arguments, `nn.PReLU()` uses a single\u001b[39;00m\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;124;03m    parameter :math:`a` across all input channels. If called with `nn.PReLU(nChannels)`,\u001b[39;00m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;124;03m    a separate :math:`a` is used for each input channel.\u001b[39;00m\n\u001b[1;32m   1317\u001b[0m \n\u001b[1;32m   1318\u001b[0m \n\u001b[1;32m   1319\u001b[0m \u001b[38;5;124;03m    .. note::\u001b[39;00m\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;124;03m        weight decay should not be used when learning :math:`a` for good performance.\u001b[39;00m\n\u001b[1;32m   1321\u001b[0m \n\u001b[1;32m   1322\u001b[0m \u001b[38;5;124;03m    .. note::\u001b[39;00m\n\u001b[1;32m   1323\u001b[0m \u001b[38;5;124;03m        Channel dim is the 2nd dim of input. When input has dims < 2, then there is\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;124;03m        no channel dim and the number of channels = 1.\u001b[39;00m\n\u001b[1;32m   1325\u001b[0m \n\u001b[1;32m   1326\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;124;03m        num_parameters (int): number of :math:`a` to learn.\u001b[39;00m\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;124;03m            Although it takes an int as input, there is only two values are legitimate:\u001b[39;00m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;124;03m            1, or the number of channels at input. Default: 1\u001b[39;00m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;124;03m        init (float): the initial value of :math:`a`. Default: 0.25\u001b[39;00m\n\u001b[1;32m   1331\u001b[0m \n\u001b[1;32m   1332\u001b[0m \u001b[38;5;124;03m    Shape:\u001b[39;00m\n\u001b[1;32m   1333\u001b[0m \u001b[38;5;124;03m        - Input: :math:`( *)` where `*` means, any number of additional\u001b[39;00m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;124;03m          dimensions.\u001b[39;00m\n\u001b[1;32m   1335\u001b[0m \u001b[38;5;124;03m        - Output: :math:`(*)`, same shape as the input.\u001b[39;00m\n\u001b[1;32m   1336\u001b[0m \n\u001b[1;32m   1337\u001b[0m \u001b[38;5;124;03m    Attributes:\u001b[39;00m\n\u001b[1;32m   1338\u001b[0m \u001b[38;5;124;03m        weight (Tensor): the learnable weights of shape (:attr:`num_parameters`).\u001b[39;00m\n\u001b[1;32m   1339\u001b[0m \n\u001b[1;32m   1340\u001b[0m \u001b[38;5;124;03m    .. image:: ../scripts/activation_images/PReLU.png\u001b[39;00m\n\u001b[1;32m   1341\u001b[0m \n\u001b[1;32m   1342\u001b[0m \u001b[38;5;124;03m    Examples::\u001b[39;00m\n\u001b[1;32m   1343\u001b[0m \n\u001b[1;32m   1344\u001b[0m \u001b[38;5;124;03m        >>> m = nn.PReLU()\u001b[39;00m\n\u001b[1;32m   1345\u001b[0m \u001b[38;5;124;03m        >>> input = torch.randn(2)\u001b[39;00m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;124;03m        >>> output = m(input)\u001b[39;00m\n\u001b[1;32m   1347\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1348\u001b[0m     __constants__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m   1349\u001b[0m     num_parameters: \u001b[38;5;28mint\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 1, 16, 16]' is invalid for input of size 4096"
     ]
    }
   ],
   "source": [
    "min_validation_loss = float(\"inf\")\n",
    "eps = 5\n",
    "best_model_so_far = None\n",
    "\n",
    "for ep in range(1, eps+1):\n",
    "    ep_time_start = time.time()\n",
    "    train_model()\n",
    "    validation_loss = eval_model(transformer_model, validation_data)\n",
    "    print()\n",
    "    print(f\"epoch {ep}, validation loss {validation_loss:.2f}, validation perplexity {math.exp(validation_loss):.2f}\")\n",
    "    print()\n",
    "\n",
    "    if validation_loss < min_validation_loss:\n",
    "        min_validation_loss = validation_loss\n",
    "        best_model_so_far = transformer_model\n",
    "    sched_module.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac22f42-41f0-4111-ba0d-30e55f91043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_loss = eval_model(best_model_so_far, testing_data)\n",
    "print(f\"testing loss {testing_loss:.2f}, testing perplexity {math.exp(testing_loss):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
